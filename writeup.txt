In order to communicate between the worker and web roles, I used a few Azure Queues. The state of the crawler, the URL’s to be processed, the start/stop command, the last ten crawled, and any errors are all stored using queues. The URL’s themselves are stored in an Azure Table, along with a counter that keeps track of how many have been inserted. These URL’s are inserted using a class where the partition key is the MD5 representation of the URL, and the row key is the URL without any special characters made using a regular expression. The CPU usage is also stored in a separate table as a class. The dashboard page uses AJAX calls to get the information that is stored in these queues and tables, and waits until all of these calls have been completed before adding them to the HTML page. For the robots.txt section, I created blacklists of where the crawler shouldn’t go using a hash set, to make it faster to look up every time before adding to the URL queue. Going through the sitemaps, I used an XML doc where the namespace manager contains the special tags used in the sitemaps on CNN and Bleacher Report. When going through the links in the queue, I used an HTML doc from the HTML Agility Pack. I first check if the URL is valid, and then proceed to grab the title, and date from the page if they exist. I then go through all of the <a> tags and grab the links on the page. If the links are to sites under CNN or Bleacher Report, I add them to the queue to be processed, and if not, they are ignored.